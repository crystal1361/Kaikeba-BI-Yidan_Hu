{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('used_car_train_20200313.zip', sep=' ')\n",
    "test_data = pd.read_csv('used_car_testB_20200421.zip', sep=' ')\n",
    "\n",
    "print(f'train data shape: {train_data.shape}')\n",
    "print(f'test data shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "- I referenced the notebook in Tianchi, ['Task2'](https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.3.1cd837c9giLdfj&postId=95457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature type and missing value check**\n",
    "- features:'model', 'bodyType', 'fuelType' and 'gearbox', have some missing values.\n",
    "- we may impute the missing values or leave them for tree-based models as they can learn the missing-value patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = msno.matrix(train_data.sample(1000),figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = msno.matrix(test_data.sample(1000), figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the notRepariedDamage feature which has unreasonable value, and mark it down as 'NaN' value\n",
    "train_data.notRepairedDamage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['notRepairedDamage'].replace('-', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.notRepairedDamage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['notRepairedDamage'].replace('-', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two features, 'seller' and 'offerType', are useless\n",
    "# and they will be deleted in the modeling stage\n",
    "train_data.seller.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.offerType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**analyze numerical and categorical features**\n",
    "\n",
    "***1. Numerical features***\n",
    "  - the price has strong correlation with kilometer, v_0, v_3, v_8 and v_12\n",
    "  - some features are also highly correlated, e.g. v_3 and v_12, such that we may only keep either one as input into the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['power', 'kilometer', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13','v_14' ]\n",
    "\n",
    "categorical_features = ['name', 'model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'notRepairedDamage', 'regionCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. numerical features correlation analysis\n",
    "correlation_numeric = train_data.loc[:, numeric_features + ['price']].corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "ax = sns.heatmap(correlation_numeric, square=True, annot=True)\n",
    "ax.set_title('correlation among numerical features and target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the target variable, 'price'\n",
    "# the target variable is highly left-skewed, such that some transformation may be needed.\n",
    "ax = sns.distplot(train_data.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution for each numerical feature\n",
    "# the 'power' variable has some values more than 600 which should be clipped\n",
    "f = pd.melt(train_data, value_vars=numeric_features)\n",
    "g = sns.FacetGrid(f, col=\"variable\", col_wrap=6, sharex=False, sharey=False)\n",
    "g = g.map(sns.distplot, \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.categorical features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[:, categorical_features].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the box_plot for some categorical features to see if they good predictors for the price target\n",
    "categorical_features_plot =  ['brand',\n",
    " 'bodyType',\n",
    " 'fuelType',\n",
    " 'gearbox']\n",
    "\n",
    "plot_data = train_data.copy()\n",
    "for c in categorical_features_plot:\n",
    "    plot_data[c] = plot_data[c].astype('category')\n",
    "    if plot_data[c].isnull().any():\n",
    "        plot_data[c] = plot_data[c].cat.add_categories(['MISSING'])\n",
    "        plot_data[c] = plot_data[c].fillna('MISSING')\n",
    "\n",
    "def boxplot(x, y, **kwargs):\n",
    "    sns.boxplot(x=x, y=y)\n",
    "    x=plt.xticks(rotation=90)\n",
    "\n",
    "f = pd.melt(plot_data, id_vars=['price'], value_vars=categorical_features_plot)\n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\n",
    "g = g.map(boxplot, \"value\", \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering/Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2.1. creat new features for days and years the car owned.\n",
    "train_data['used_days'] = (pd.to_datetime(train_data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(train_data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "\n",
    "test_data['used_days'] = (pd.to_datetime(test_data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                            pd.to_datetime(test_data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "\n",
    "train_data['used_year'] = train_data['used_days'] / 365\n",
    "\n",
    "test_data['used_year'] = test_data['used_days'] / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2.2. build some discrete features from numeric ones using bins.\n",
    "train_data['power'] = np.clip(train_data.power, 0, 600)\n",
    "test_data['power'] = np.clip(test_data.power, 0, 600)\n",
    "\n",
    "ax = train_data.power.hist(bins=50)\n",
    "ax.set_xlabel('power')\n",
    "ax.set_title('power distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_bin = [i*10 for i in range(31)]\n",
    "train_data['power_bin'] = pd.cut(train_data.power, bins=power_bin, labels=False)\n",
    "test_data['power_bin'] = pd.cut(test_data.power, bins=power_bin, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train_data.model.hist(bins=30)\n",
    "ax.set_xlabel('model')\n",
    "ax.set_title('model distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = [i*10 for i in range(14)]\n",
    "train_data['model_bin'] = pd.cut(train_data.model, bins=model_bin, labels=False)\n",
    "test_data['model_bin'] = pd.cut(test_data.model, bins=model_bin, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2.3. build some cross-interaction features\n",
    "def cross_interaction(train_data, cat_feature, num_feature, test_data):\n",
    "    column_name = {'max':cat_feature+'_'+num_feature+'_max', \n",
    "              'median':cat_feature+'_'+num_feature+'_median', \n",
    "              'min':cat_feature+'_'+num_feature+'_min', \n",
    "              'sum':cat_feature+'_'+num_feature+'_sum', \n",
    "              'std':cat_feature+'_'+num_feature+'_std', \n",
    "              'mean':cat_feature+'_'+num_feature+'_mean'}\n",
    "    cros_feature = train_data.groupby(cat_feature)[num_feature].agg(['max', 'median', 'min', 'sum', 'std', 'mean'])\\\n",
    "                                                            .reset_index().rename(columns=column_name)\n",
    "    train_data = train_data.merge(cros_feature, how='left', on=cat_feature)\n",
    "    test_data = test_data.merge(cros_feature, how='left', on=cat_feature)\n",
    "    \n",
    "    return train_data, test_data\n",
    "    \n",
    "train_data, test_data = cross_interaction(train_data, 'brand', 'price', test_data)\n",
    "train_data, test_data = cross_interaction(train_data, 'bodyType', 'price', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.4. drop non-significant features\n",
    "features_to_drop = ['name', 'regDate', 'regionCode', 'seller', 'offerType', 'creatDate']\n",
    "\n",
    "train_data = train_data.drop(columns=features_to_drop)\n",
    "test_data = test_data.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.5 transfer the target variable\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "power_trans = PowerTransformer(method='box-cox')\n",
    "train_data['price_trasfer'] = power_trans.fit_transform(train_data.loc[:,['price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(train_data.price_trasfer)\n",
    "ax.set_title('transfered price variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "- additional analyses will be performed in the future to imporve predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.notRepairedDamage = train_data.notRepairedDamage.astype(float)\n",
    "test_data.notRepairedDamage = test_data.notRepairedDamage.astype(float)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=1500, learning_rate=0.05)\n",
    "scores = cross_val_score(xgb_model, X=train_data.drop(columns=['SaleID', 'price', 'price_trasfer']), y=train_data.price, cv=5, scoring=make_scorer(mean_absolute_error))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRFRegressor(n_estimators=1500, learning_rate=0.1)\n",
    "xgb_model.fit(X=train_data.drop(columns=['SaleID', 'price', 'price_trasfer']), y=train_data.price)\n",
    "\n",
    "pred_price = xgb_model.predict(data=test_data.drop(columns='SaleID'))\n",
    "prediction_price = test_data.loc[:, ['SaleID']]\n",
    "prediction_price['price'] = pred_price\n",
    "\n",
    "prediction_price.to_csv('used_car_testB_submit_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(n_estimators=1500, learning_rate=0.05)\n",
    "scores = cross_val_score(lgb_model, X=train_data.drop(columns=['SaleID', 'price', 'price_trasfer']), y=train_data.price, cv=5, scoring=make_scorer(mean_absolute_error))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(n_estimators=1500, learning_rate=0.05)\n",
    "lgb_model.fit(X=train_data.drop(columns=['SaleID', 'price', 'price_trasfer']), y=train_data.price_trasfer)\n",
    "\n",
    "pred_price = lgb_model.predict(X=test_data.drop(columns='SaleID'))\n",
    "prediction_price = test_data.loc[:, ['SaleID']]\n",
    "prediction_price['price'] = power_trans.inverse_transform(pred_price.reshape(-1, 1))\n",
    "\n",
    "prediction_price.to_csv('used_car_testB_submit_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_missing  = train_data.dropna()\n",
    "test_data_no_missing = test_data.dropna()\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train_data_no_missing.drop(columns=['SaleID', 'price', 'price_trasfer']), y=train_data_no_missing.price)\n",
    "\n",
    "pred_price = rf.predict(X=test_data_no_missing.drop(columns='SaleID'))\n",
    "prediction_price = test_data_no_missing.loc[:, ['SaleID']]\n",
    "prediction_price['price'] = pred_price\n",
    "\n",
    "prediction_price.to_csv('used_car_testB_submit_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(train_data_no_missing.drop(columns=['SaleID', 'price', 'price_trasfer']), y=train_data_no_missing.price)\n",
    "\n",
    "pred_price = lg.predict(X=test_data_no_missing.drop(columns='SaleID'))\n",
    "prediction_price = test_data_no_missing.loc[:, ['SaleID']]\n",
    "prediction_price['price'] = pred_price\n",
    "\n",
    "prediction_price.to_csv('used_car_testB_submit_lg.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('py37_r': conda)",
   "language": "python",
   "name": "python37764bitpy37rconda46133dc75c394336a8fdf331ecb0c8bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
